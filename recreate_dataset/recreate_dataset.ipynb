{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook with steps to recreate the dataset\n",
    "\n",
    "1. Get the commments list from the dataset_comment_ids.txt\n",
    "2. Download the comments using the YouTube Data API \n",
    "3. Use the labels to complete the data preprocessing \n",
    "\n",
    "### Step 1\n",
    "\n",
    "You can authenticate with the Google API in various ways (using OAuth, keys etc)\n",
    "We have provided the code to download the comments, all you need is a key. \n",
    "A key can be obtained by opening a project in your google developer console and \n",
    "enabling the YouTube Data API for that project. \n",
    "\n",
    "The comments are stored in comment_ids.txt\n",
    "The following cell runs get_comments_from_comment_ids.py and save them in the folder 'raw_comments/'. Obtain a key and save it in keys.txt to download the comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# keys.txt should contain 1 key in each line \n",
    "# comment_ids.txt contains the comment IDs \n",
    "# raw_comments/ is the save folder\n",
    "\n",
    "!python get_comments_from_comment_ids.py keys.txt comment_ids.txt raw_comments/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 \n",
    "\n",
    "1. Load comments from raw_comments/ \n",
    "2. Verify sha1 hash\n",
    "3. Match with labelling info (labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import missingno as msno \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from pprint import pprint\n",
    "import random, math\n",
    "from collections import Counter\n",
    "from itertools import chain \n",
    "from utils import *\n",
    "import json, os, sys \n",
    "from collections import defaultdict\n",
    "import hashlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_list = os.listdir(\"raw_comments/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Retrieve comments and check sha1 hash value \n",
    "\n",
    "\n",
    "all_comments_retrieved = {}\n",
    "sha1 = hashlib.sha1() \n",
    "\n",
    "with open(\"comment_ids_with_hash.json\")  as fp : \n",
    "    doc = json.load(fp)\n",
    "\n",
    "\n",
    "for comment in comment_list: \n",
    "    comment_filepath = os.path.join(\"raw_comments\",comment)\n",
    "    with open(comment_filepath, encoding='utf-8') as fp : \n",
    "        \n",
    "        comment_doc = json.load(fp)\n",
    "        if comment_doc[comment] != \"Comment has been removed by the user. To consruct full dataset, contact authors.\" : \n",
    "            comment_text = comment_doc[comment] \n",
    "            comment_hash = hashlib.sha1(comment_text.encode('utf-8'))\n",
    "            digest = comment_hash.hexdigest() \n",
    "            if digest != doc[comment] : \n",
    "                print(f\"The comment {comment} has been removed or modified since the collection of this dataset, contact authors for full data.\")\n",
    "                \n",
    "            else : \n",
    "                all_comments_retrieved[comment] = comment_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_comments_retrieved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split comments into sentences \n",
    "\n",
    "all_comments_split = {}\n",
    "\n",
    "for cid, comment in all_comments_retrieved.items():\n",
    "    all_comments_split[cid] = shrink_delimiters(remove_emoji(remove_punc(comment)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labelling info \n",
    "\n",
    "labels_df = pd.read_csv('labelling_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df[[\"actual_comment_ids\",\"attribution\",\"primary_attribution\",\"secondary_attribution\",\"multiple_attribution_1\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sentences with labelling info to make the dataset\n",
    "\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for index, row in labels_df.iterrows() : \n",
    "    comment_id = \".\".join(row.actual_comment_ids.split(\".\")[:-1])\n",
    "    sentence_index = int(row.actual_comment_ids.split(\".\")[-1])\n",
    "    \n",
    "    if comment_id in all_comments_split.keys():\n",
    "        #print(all_comments_split[comment_id], \"\\n\", sentence_ids)\n",
    "        sentences.append(all_comments_split[comment_id][sentence_index])\n",
    "        \n",
    "    else : \n",
    "        sentences.append(np.nan)\n",
    "        \n",
    "labels_df[\"sentence\"] = sentences\n",
    "\n",
    "print(f\"{labels_df['sentence'].isnull().sum()} sentences were not found\")\n",
    "labels_df = labels_df.dropna(subset=[\"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df[[\"actual_comment_ids\",\"sentence\",\"attribution\",\"primary_attribution\",\"secondary_attribution\",\"multiple_attribution_1\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep information about all topics in a comment - \n",
    "\n",
    "topics_sets = {}\n",
    "\n",
    "unique_comments = set(labels_df.comment_id)\n",
    "\n",
    "for comment in unique_comments :\n",
    "    df_temp_sentences = list(labels_df[(labels_df.comment_id == comment) & (labels_df.attribution == 0)].sentence)\n",
    "    df_temp = labels_df.loc[labels_df.comment_id == comment][['primary_attribution','secondary_attribution','multiple_attribution_1','other_attribution']]\n",
    "    temp_topic_set = set([x for x in chain.from_iterable(df_temp.values) if not type(x) == float])\n",
    "    #print(temp_topic_set)\n",
    "    for sent in df_temp_sentences :\n",
    "        topics_sets[sent] = temp_topic_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 \n",
    "\n",
    "1. Chain consecutive sentences of the same comments having same attribution factor. \n",
    "2. Save labels for testing. This labels are later used to measure model perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain consecutive comments with the same attributed factor  \n",
    "\n",
    "chains = make_comments(labels_df)\n",
    "\n",
    "#remove chains with less than 4 words\n",
    "\n",
    "chains_final = [x for x in chains if len(x[0].split()) >= 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chains_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final dataframe, add negative labels, drop dupes \n",
    "\n",
    "chains_df = pd.DataFrame(data=chains_final, columns=['sentences','attribution','attrib_words'])\n",
    "\n",
    "#chains_df.head()\n",
    "chains_df_replaced = replace_no_attrib(chains_df, topics_sets)\n",
    "chains_df_replaced.drop_duplicates(inplace=True, keep=False)\n",
    "\n",
    "print(\"Total number of chains\", chains_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(chains_df[chains_df.attribution == 1].shape) \n",
    "# print(chains_df[chains_df.attribution == 0].shape) \n",
    "\n",
    "from collections import defaultdict \n",
    "topic_sanity = defaultdict(list)\n",
    "\n",
    "for index, row in chains_df_replaced.iterrows() : \n",
    "    if row.attribution == 1 : \n",
    "        topic_sanity[row.sentences].append(row.attrib_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 \n",
    "\n",
    "Save the label info and the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chains_df_replaced.to_csv(\"final_dataset.csv\",index=False)\n",
    "\n",
    "with open('topic_sanity.json', 'w') as fp : \n",
    "    json.dump(topic_sanity, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3] *",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
